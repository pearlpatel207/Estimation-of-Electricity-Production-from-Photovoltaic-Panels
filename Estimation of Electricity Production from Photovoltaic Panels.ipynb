{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation of Electricity Production from Photovoltaic Panels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the use of LSTM based Neural Network for estimation of electricity produced from Photovoltaic Panels based on weather data.\n",
    "<br>\n",
    "This is covered in two parts:\n",
    "<br>\n",
    "- Prediction of Irradiance\n",
    "- Prediction of Power Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset contains weather data such as Temperature, Pressure, Humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "\n",
    "PATH = 'PATH_TO_INPUT FILE'\n",
    "\n",
    "df = pd.read_csv(PATH)\n",
    "df['TimeStamp'] = pd.to_datetime(df['TimeStamp'])\n",
    "df['year'] = df['TimeStamp'].dt.year\n",
    "df['date'] = df['TimeStamp'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data\n",
    "Data is divided into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[(df.year < 2015)]\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df.year == 2015]\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "Data is scaled using the Min-Max Scaler. All the features are scaled between the range [0,1]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the data\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled = train\n",
    "test_scaled = test\n",
    "\n",
    "train_scaled[['sunHour', 'uvIndex.1', 'FeelsLikeC', 'HeatIndexC', 'cloudcover', 'humidity', 'pressure', 'tempC', 'visibility', 'day_of_year','hour_of_day']] = scaler.fit_transform(train[['sunHour', 'uvIndex.1', 'FeelsLikeC', 'HeatIndexC', 'cloudcover', 'humidity', 'pressure', 'tempC', 'visibility', 'day_of_year','hour_of_day']])\n",
    "test_scaled[['sunHour', 'uvIndex.1', 'FeelsLikeC', 'HeatIndexC', 'cloudcover', 'humidity', 'pressure', 'tempC', 'visibility', 'day_of_year','hour_of_day']] = scaler.transform(test[['sunHour', 'uvIndex.1', 'FeelsLikeC', 'HeatIndexC', 'cloudcover', 'humidity', 'pressure', 'tempC', 'visibility', 'day_of_year','hour_of_day']])\n",
    "\n",
    "yscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled[['dc_pow']] = yscaler.fit_transform(train[['dc_pow']])\n",
    "test_scaled[['dc_pow']] = yscaler.transform(test[['dc_pow']])\n",
    "\n",
    "Irrscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled[['Irr']] = Irrscaler.fit_transform(train[['Irr']])\n",
    "test_scaled[['Irr']] = Irrscaler.transform(test[['Irr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating features and labels for irradiance prediction\n",
    "\n",
    "#training data\n",
    "trainf = train_scaled[['uvIndex.1', 'cloudcover', 'humidity', 'tempC', 'visibility', 'day_of_year','hour_of_day',  'Irr']].copy()\n",
    "traint = train_scaled[['Irr']]\n",
    "\n",
    "train_dataset = trainf.values\n",
    "train_target = traint.values\n",
    "\n",
    "#testing data\n",
    "testf = test_scaled[['uvIndex.1', 'cloudcover', 'humidity', 'tempC', 'visibility', 'day_of_year','hour_of_day', 'Irr']].copy()\n",
    "testt = test_scaled[['Irr']]\n",
    "\n",
    "test_dataset = testf.values\n",
    "test_target = testt.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing the Dataset\n",
    "The data is windowed into input and output components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_dataset(dataset, target, history_size,\n",
    "                      target_size):\n",
    "    \n",
    "    '''\n",
    "    The LSTM model makes predictions (target) based on a window of consecutive samples from the data (dataset)\n",
    "    history_size specifies the number of past samples to be considered for predictions\n",
    "    target_size specifies the time offset between past sample and predictions\n",
    "    '''\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(history_size, len(dataset)-target_size):\n",
    "        indices = range(i-history_size, i, 1)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        labels.append(target[i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY = 1\n",
    "TARGET = 0\n",
    "\n",
    "x_train, y_train = window_dataset(train_dataset, train_target, HISTORY, TARGET)\n",
    "x_test, y_test = window_dataset(test_dataset, test_target, HISTORY, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single window of past history : (1, 8)\n"
     ]
    }
   ],
   "source": [
    "#convert the data into TensorFlow Dataset to feed it into TensorFlow Model\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "print ('Single window of past history : {}'.format(x_train[0].shape))\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_data = val_data.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM based Neural Network for Irradiance Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network for irradiance prediction\n",
    "EPOCHS = 300\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=60, min_delta=0.0001, verbose=1,restore_best_weights=True)\n",
    "\n",
    "Irr_model = tf.keras.models.Sequential()\n",
    "Irr_model.add(tf.keras.layers.LSTM(32, input_shape=x_train.shape[-2:], return_sequences=True))\n",
    "Irr_model.add(tf.keras.layers.LSTM(32, activation=\"relu\"))\n",
    "Irr_model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n",
    "Irr_model.add(tf.keras.layers.Dense(8, activation=\"relu\"))\n",
    "Irr_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "Irr_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/300\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.0757 - val_loss: 0.0380\n",
      "Epoch 2/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0446 - val_loss: 0.0357\n",
      "Epoch 3/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0429 - val_loss: 0.0304\n",
      "Epoch 4/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0370 - val_loss: 0.0300\n",
      "Epoch 5/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0430 - val_loss: 0.0291\n",
      "Epoch 6/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0340 - val_loss: 0.0295\n",
      "Epoch 7/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0402 - val_loss: 0.0281\n",
      "Epoch 8/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0409 - val_loss: 0.0341\n",
      "Epoch 9/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0344 - val_loss: 0.0283\n",
      "Epoch 10/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0398 - val_loss: 0.0278\n",
      "Epoch 11/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0356 - val_loss: 0.0306\n",
      "Epoch 12/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0364 - val_loss: 0.0291\n",
      "Epoch 13/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0400 - val_loss: 0.0283\n",
      "Epoch 14/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0327 - val_loss: 0.0268\n",
      "Epoch 15/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0383 - val_loss: 0.0286\n",
      "Epoch 16/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0394 - val_loss: 0.0299\n",
      "Epoch 17/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0330 - val_loss: 0.0257\n",
      "Epoch 18/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0397 - val_loss: 0.0257\n",
      "Epoch 19/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0319 - val_loss: 0.0262\n",
      "Epoch 20/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0375 - val_loss: 0.0261\n",
      "Epoch 21/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0388 - val_loss: 0.0258\n",
      "Epoch 22/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0323 - val_loss: 0.0249\n",
      "Epoch 23/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0377 - val_loss: 0.0298\n",
      "Epoch 24/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0353 - val_loss: 0.0268\n",
      "Epoch 25/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0340 - val_loss: 0.0305\n",
      "Epoch 26/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0389 - val_loss: 0.0286\n",
      "Epoch 27/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0309 - val_loss: 0.0286\n",
      "Epoch 28/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0373 - val_loss: 0.0258\n",
      "Epoch 29/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0386 - val_loss: 0.0259\n",
      "Epoch 30/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0317 - val_loss: 0.0251\n",
      "Epoch 31/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0382 - val_loss: 0.0255\n",
      "Epoch 32/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0311 - val_loss: 0.0262\n",
      "Epoch 33/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0365 - val_loss: 0.0271\n",
      "Epoch 34/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0377 - val_loss: 0.0259\n",
      "Epoch 35/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0315 - val_loss: 0.0272TA: 0s - lo\n",
      "Epoch 36/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0364 - val_loss: 0.0251\n",
      "Epoch 37/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0358 - val_loss: 0.0278\n",
      "Epoch 38/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0328 - val_loss: 0.0254\n",
      "Epoch 39/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0382 - val_loss: 0.0248\n",
      "Epoch 40/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0301 - val_loss: 0.0254\n",
      "Epoch 41/300\n",
      "200/200 [==============================] - 4s 19ms/step - loss: 0.0366 - val_loss: 0.0264\n",
      "Epoch 42/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0381 - val_loss: 0.0258\n",
      "Epoch 43/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0312 - val_loss: 0.0254\n",
      "Epoch 44/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0373 - val_loss: 0.0260\n",
      "Epoch 45/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0312 - val_loss: 0.0245\n",
      "Epoch 46/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0353 - val_loss: 0.0275\n",
      "Epoch 47/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0371 - val_loss: 0.0244\n",
      "Epoch 48/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0309 - val_loss: 0.0259\n",
      "Epoch 49/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0361 - val_loss: 0.0254\n",
      "Epoch 50/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0365 - val_loss: 0.0268\n",
      "Epoch 51/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0314 - val_loss: 0.0263\n",
      "Epoch 52/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0383 - val_loss: 0.0238\n",
      "Epoch 53/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0296 - val_loss: 0.0265\n",
      "Epoch 54/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0361 - val_loss: 0.0242\n",
      "Epoch 55/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0377 - val_loss: 0.0281\n",
      "Epoch 56/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0309 - val_loss: 0.0269\n",
      "Epoch 57/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0362 - val_loss: 0.0262\n",
      "Epoch 58/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0326 - val_loss: 0.0254\n",
      "Epoch 59/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0338 - val_loss: 0.0240\n",
      "Epoch 60/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0372 - val_loss: 0.0261\n",
      "Epoch 61/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0304 - val_loss: 0.0253\n",
      "Epoch 62/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0359 - val_loss: 0.0247\n",
      "Epoch 63/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0368 - val_loss: 0.0241\n",
      "Epoch 64/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0309 - val_loss: 0.0250\n",
      "Epoch 65/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0379 - val_loss: 0.0240\n",
      "Epoch 66/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0293 - val_loss: 0.0255\n",
      "Epoch 67/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0358 - val_loss: 0.0244\n",
      "Epoch 68/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0371 - val_loss: 0.0242\n",
      "Epoch 69/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0306 - val_loss: 0.0247\n",
      "Epoch 70/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0361 - val_loss: 0.0252\n",
      "Epoch 71/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0331 - val_loss: 0.0243\n",
      "Epoch 72/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0331 - val_loss: 0.0284\n",
      "Epoch 73/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0370 - val_loss: 0.0250\n",
      "Epoch 74/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0297 - val_loss: 0.0253\n",
      "Epoch 75/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0356 - val_loss: 0.0250\n",
      "Epoch 76/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0373 - val_loss: 0.0246\n",
      "Epoch 77/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0305 - val_loss: 0.0252\n",
      "Epoch 78/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0372 - val_loss: 0.0275 - loss: 0.03 - ETA: 0s - lo\n",
      "Epoch 79/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0297 - val_loss: 0.0259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0352 - val_loss: 0.0264\n",
      "Epoch 81/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0366 - val_loss: 0.0254\n",
      "Epoch 82/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0305 - val_loss: 0.0262\n",
      "Epoch 83/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0356 - val_loss: 0.0250\n",
      "Epoch 84/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0342 - val_loss: 0.0255\n",
      "Epoch 85/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0320 - val_loss: 0.0244\n",
      "Epoch 86/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0371 - val_loss: 0.0253\n",
      "Epoch 87/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0293 - val_loss: 0.0238\n",
      "Epoch 88/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0357 - val_loss: 0.0259\n",
      "Epoch 89/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0372 - val_loss: 0.0245\n",
      "Epoch 90/300\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.0300 - val_loss: 0.0260: 0s - loss: 0.\n",
      "Epoch 91/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0368 - val_loss: 0.0245\n",
      "Epoch 92/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0300 - val_loss: 0.0236\n",
      "Epoch 93/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0347 - val_loss: 0.0246\n",
      "Epoch 94/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0363 - val_loss: 0.0233\n",
      "Epoch 95/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0302 - val_loss: 0.0246\n",
      "Epoch 96/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0355 - val_loss: 0.0259\n",
      "Epoch 97/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0352 - val_loss: 0.0253\n",
      "Epoch 98/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0311 - val_loss: 0.0243\n",
      "Epoch 99/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0371 - val_loss: 0.0234\n",
      "Epoch 100/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0290 - val_loss: 0.0241\n",
      "Epoch 101/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0355 - val_loss: 0.0249\n",
      "Epoch 102/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0370 - val_loss: 0.0238\n",
      "Epoch 103/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0300 - val_loss: 0.0249\n",
      "Epoch 104/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0362 - val_loss: 0.0268\n",
      "Epoch 105/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0310 - val_loss: 0.0239\n",
      "Epoch 106/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0339 - val_loss: 0.0257\n",
      "Epoch 107/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0361 - val_loss: 0.0242\n",
      "Epoch 108/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0299 - val_loss: 0.0245\n",
      "Epoch 109/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0353 - val_loss: 0.0257\n",
      "Epoch 110/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0361 - val_loss: 0.0241\n",
      "Epoch 111/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0304 - val_loss: 0.0247\n",
      "Epoch 112/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0372 - val_loss: 0.0246\n",
      "Epoch 113/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0288 - val_loss: 0.0235\n",
      "Epoch 114/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0353 - val_loss: 0.0238\n",
      "Epoch 115/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0365 - val_loss: 0.0258\n",
      "Epoch 116/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0302 - val_loss: 0.0241\n",
      "Epoch 117/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0354 - val_loss: 0.0258\n",
      "Epoch 118/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0322 - val_loss: 0.0250\n",
      "Epoch 119/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0329 - val_loss: 0.0232\n",
      "Epoch 120/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0366 - val_loss: 0.0246\n",
      "Epoch 121/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0294 - val_loss: 0.0241\n",
      "Epoch 122/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0352 - val_loss: 0.0252\n",
      "Epoch 123/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0363 - val_loss: 0.0242\n",
      "Epoch 124/300\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0301 - val_loss: 0.0257\n",
      "Epoch 125/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0369 - val_loss: 0.0253\n",
      "Epoch 126/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0290 - val_loss: 0.0254\n",
      "Epoch 127/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0349 - val_loss: 0.0238\n",
      "Epoch 128/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0362 - val_loss: 0.0241\n",
      "Epoch 129/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0301 - val_loss: 0.0238\n",
      "Epoch 130/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0350 - val_loss: 0.0237\n",
      "Epoch 131/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0334 - val_loss: 0.0233\n",
      "Epoch 132/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0318 - val_loss: 0.0238\n",
      "Epoch 133/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0366 - val_loss: 0.0249\n",
      "Epoch 134/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0290 - val_loss: 0.0238\n",
      "Epoch 135/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0352 - val_loss: 0.0263\n",
      "Epoch 136/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0366 - val_loss: 0.0237\n",
      "Epoch 137/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0299 - val_loss: 0.0260\n",
      "Epoch 138/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0364 - val_loss: 0.0254\n",
      "Epoch 139/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0292 - val_loss: 0.0249\n",
      "Epoch 140/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0348 - val_loss: 0.0278\n",
      "Epoch 141/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0359 - val_loss: 0.0240ETA: 1s - loss: \n",
      "Epoch 142/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0298 - val_loss: 0.0242\n",
      "Epoch 143/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0351 - val_loss: 0.0244\n",
      "Epoch 144/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0346 - val_loss: 0.0241\n",
      "Epoch 145/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0309 - val_loss: 0.0276\n",
      "Epoch 146/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0369 - val_loss: 0.0235\n",
      "Epoch 147/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0285 - val_loss: 0.0230\n",
      "Epoch 148/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0351 - val_loss: 0.0258\n",
      "Epoch 149/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0366 - val_loss: 0.0244\n",
      "Epoch 150/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0298 - val_loss: 0.0237\n",
      "Epoch 151/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0360 - val_loss: 0.0251\n",
      "Epoch 152/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0301 - val_loss: 0.0260\n",
      "Epoch 153/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0337 - val_loss: 0.0234\n",
      "Epoch 154/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0359 - val_loss: 0.0250\n",
      "Epoch 155/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0297 - val_loss: 0.0243\n",
      "Epoch 156/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0350 - val_loss: 0.0256\n",
      "Epoch 157/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0351 - val_loss: 0.0250\n",
      "Epoch 158/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0303 - val_loss: 0.0263\n",
      "Epoch 159/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0369 - val_loss: 0.0256\n",
      "Epoch 160/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0285 - val_loss: 0.0245\n",
      "Epoch 161/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0349 - val_loss: 0.0243\n",
      "Epoch 162/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0363 - val_loss: 0.0230\n",
      "Epoch 163/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0298 - val_loss: 0.0257\n",
      "Epoch 164/300\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0356 - val_loss: 0.0243\n",
      "Epoch 165/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0311 - val_loss: 0.0235\n",
      "Epoch 166/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0329 - val_loss: 0.0247\n",
      "Epoch 167/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0360 - val_loss: 0.0254: 0s \n",
      "Epoch 168/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0292 - val_loss: 0.0237\n",
      "Epoch 169/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0349 - val_loss: 0.0254\n",
      "Epoch 170/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0361 - val_loss: 0.0230\n",
      "Epoch 171/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0295 - val_loss: 0.0245\n",
      "Epoch 172/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0370 - val_loss: 0.0252\n",
      "Epoch 173/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0285 - val_loss: 0.0239\n",
      "Epoch 174/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0347 - val_loss: 0.0257\n",
      "Epoch 175/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0362 - val_loss: 0.0242\n",
      "Epoch 176/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0298 - val_loss: 0.0233\n",
      "Epoch 177/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0349 - val_loss: 0.0234\n",
      "Epoch 178/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0325 - val_loss: 0.0247\n",
      "Epoch 179/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0319 - val_loss: 0.0262\n",
      "Epoch 180/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0362 - val_loss: 0.0240\n",
      "Epoch 181/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0288 - val_loss: 0.0234\n",
      "Epoch 182/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0349 - val_loss: 0.0237\n",
      "Epoch 183/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0363 - val_loss: 0.0240\n",
      "Epoch 184/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0298 - val_loss: 0.0243\n",
      "Epoch 185/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0362 - val_loss: 0.0240\n",
      "Epoch 186/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0289 - val_loss: 0.0236\n",
      "Epoch 187/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0346 - val_loss: 0.0244\n",
      "Epoch 188/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0358 - val_loss: 0.0231\n",
      "Epoch 189/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0295 - val_loss: 0.0258\n",
      "Epoch 190/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0351 - val_loss: 0.0241\n",
      "Epoch 191/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0335 - val_loss: 0.0233\n",
      "Epoch 192/300\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0312 - val_loss: 0.0278\n",
      "Epoch 193/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0364 - val_loss: 0.0237\n",
      "Epoch 194/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0285 - val_loss: 0.0238\n",
      "Epoch 195/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0349 - val_loss: 0.0240\n",
      "Epoch 196/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0365 - val_loss: 0.0239\n",
      "Epoch 197/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0295 - val_loss: 0.0240\n",
      "Epoch 198/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0358 - val_loss: 0.0237\n",
      "Epoch 199/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0294 - val_loss: 0.0234\n",
      "Epoch 200/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0342 - val_loss: 0.0244\n",
      "Epoch 201/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0355 - val_loss: 0.0246\n",
      "Epoch 202/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0295 - val_loss: 0.0237\n",
      "Epoch 203/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0347 - val_loss: 0.0236\n",
      "Epoch 204/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0347 - val_loss: 0.0245\n",
      "Epoch 205/300\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0301 - val_loss: 0.0232\n",
      "Epoch 206/300\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0368 - val_loss: 0.0250\n",
      "Epoch 207/300\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.0283Restoring model weights from the end of the best epoch\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0283 - val_loss: 0.0262\n",
      "Epoch 00207: early stopping\n"
     ]
    }
   ],
   "source": [
    "Irr_history = Irr_model.fit(train_data, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=200,\n",
    "                                            validation_data=val_data,\n",
    "                                            validation_steps=50,\n",
    "                                            callbacks=[es]\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the predicted irradiace values to the dataset\n",
    "\n",
    "y = Irr_model.predict(x_test)\n",
    "Irr_cal = Irr_model.predict(x_train)\n",
    "\n",
    "test_scaled.drop([0], inplace = True)\n",
    "train_scaled.drop([0], inplace = True)\n",
    "\n",
    "test_scaled['Irr_cal'] = y\n",
    "train_scaled['Irr_cal'] = Irr_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling data for hourly predictions\n",
    "\n",
    "train_scaled = train_scaled.groupby(['date','hour_of_day']).first().reset_index()\n",
    "test_scaled = test_scaled.groupby(['date','hour_of_day']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating features and labels for hourly power production\n",
    "\n",
    "xtrain = train_scaled[['Irr_cal', 'uvIndex.1', 'tempC', 'cloudcover', 'humidity', 'day_of_year','hour_of_day', 'dc_pow']].copy()\n",
    "ytrain = train_scaled[['dc_pow']].copy()\n",
    "\n",
    "xtest = test_scaled[['Irr_cal', 'uvIndex.1', 'tempC', 'cloudcover', 'humidity', 'day_of_year','hour_of_day', 'dc_pow']].copy()\n",
    "ytest = test_scaled[['dc_pow']].copy()\n",
    "\n",
    "trainx = xtrain.values\n",
    "trainy = ytrain.values\n",
    "\n",
    "testx = xtest.values\n",
    "testy = ytest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowing dataset\n",
    "\n",
    "HISTORY = 1     #to include more historical data for better predictions, increase HISTORY value\n",
    "TARGET = 0      #for predictions of power produced 3 hours later, set TARGET = 3\n",
    "\n",
    "x_train, y_train = window_dataset(trainx, trainy, HISTORY, TARGET)\n",
    "x_test, y_test = window_dataset(testx, testy, HISTORY, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single window of past history : (1, 8)\n"
     ]
    }
   ],
   "source": [
    "#convert the data into TensorFlow Dataset to feed it into TensorFlow Model\n",
    "\n",
    "print ('Single window of past history : {}'.format(x_train[0].shape))\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_data = val_data.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM based Neural Network for Power Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM based Neural Network for hourly power production\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=30, min_delta=0.0001, verbose=1,restore_best_weights=True)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(64, input_shape=x_train.shape[-2:], return_sequences=True))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.LSTM(32, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(8, activation=\"tanh\"))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/300\n",
      "200/200 [==============================] - 28s 141ms/step - loss: 0.1530 - val_loss: 0.0968: - ETA: 3s - loss: 0.162 - ETA: 3s - loss: 0.161 - ETA: 2s - loss:\n",
      "Epoch 2/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0772 - val_loss: 0.0788\n",
      "Epoch 3/300\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0681 - val_loss: 0.0608\n",
      "Epoch 4/300\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0651 - val_loss: 0.0580\n",
      "Epoch 5/300\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0637 - val_loss: 0.0578\n",
      "Epoch 6/300\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.0636 - val_loss: 0.0581\n",
      "Epoch 7/300\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.0631 - val_loss: 0.0569\n",
      "Epoch 8/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0627 - val_loss: 0.0570\n",
      "Epoch 9/300\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0622 - val_loss: 0.0563\n",
      "Epoch 10/300\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.0624 - val_loss: 0.0563\n",
      "Epoch 11/300\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0622 - val_loss: 0.0553\n",
      "Epoch 12/300\n",
      "200/200 [==============================] - 5s 27ms/step - loss: 0.0622 - val_loss: 0.0569\n",
      "Epoch 13/300\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0617 - val_loss: 0.0572\n",
      "Epoch 14/300\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0621 - val_loss: 0.0559\n",
      "Epoch 15/300\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0621 - val_loss: 0.0583\n",
      "Epoch 16/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0618 - val_loss: 0.0551\n",
      "Epoch 17/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0619 - val_loss: 0.0582\n",
      "Epoch 18/300\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.0617 - val_loss: 0.0560\n",
      "Epoch 19/300\n",
      "200/200 [==============================] - 4s 19ms/step - loss: 0.0621 - val_loss: 0.0553\n",
      "Epoch 20/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0621 - val_loss: 0.0556\n",
      "Epoch 21/300\n",
      "200/200 [==============================] - 5s 26ms/step - loss: 0.0616 - val_loss: 0.0557\n",
      "Epoch 22/300\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.0616 - val_loss: 0.0558\n",
      "Epoch 23/300\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.0613 - val_loss: 0.0566\n",
      "Epoch 24/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0616 - val_loss: 0.0569\n",
      "Epoch 25/300\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0612 - val_loss: 0.0558\n",
      "Epoch 26/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0611 - val_loss: 0.0560\n",
      "Epoch 27/300\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0611 - val_loss: 0.0564\n",
      "Epoch 28/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0612 - val_loss: 0.0564\n",
      "Epoch 29/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0615 - val_loss: 0.0556 ETA: 0s - loss: 0.0\n",
      "Epoch 30/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0612 - val_loss: 0.0568\n",
      "Epoch 31/300\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.0612 - val_loss: 0.0553\n",
      "Epoch 32/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0610 - val_loss: 0.0551\n",
      "Epoch 33/300\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.0610 - val_loss: 0.0555\n",
      "Epoch 34/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0609 - val_loss: 0.0569\n",
      "Epoch 35/300\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.0614 - val_loss: 0.0558\n",
      "Epoch 36/300\n",
      "200/200 [==============================] - 5s 24ms/step - loss: 0.0606 - val_loss: 0.0581\n",
      "Epoch 37/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0609 - val_loss: 0.0560\n",
      "Epoch 38/300\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0609 - val_loss: 0.0552\n",
      "Epoch 39/300\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.0609 - val_loss: 0.0555\n",
      "Epoch 40/300\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.0608 - val_loss: 0.0557\n",
      "Epoch 41/300\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.0610 - val_loss: 0.0565\n",
      "Epoch 42/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0605 - val_loss: 0.0560\n",
      "Epoch 43/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0612 - val_loss: 0.0553\n",
      "Epoch 44/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0610 - val_loss: 0.0550\n",
      "Epoch 45/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0603 - val_loss: 0.0554\n",
      "Epoch 46/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0608 - val_loss: 0.0554\n",
      "Epoch 47/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0609 - val_loss: 0.0558\n",
      "Epoch 48/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0608 - val_loss: 0.0563\n",
      "Epoch 49/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0609 - val_loss: 0.0560 - loss: 0\n",
      "Epoch 50/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0607 - val_loss: 0.0548\n",
      "Epoch 51/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0604 - val_loss: 0.0553\n",
      "Epoch 52/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0605 - val_loss: 0.0557\n",
      "Epoch 53/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0608 - val_loss: 0.0557\n",
      "Epoch 54/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0608 - val_loss: 0.0569\n",
      "Epoch 55/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0609 - val_loss: 0.05660s - loss:  - ETA: 0s - loss: 0.0\n",
      "Epoch 56/300\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.0608 - val_loss: 0.0564\n",
      "Epoch 57/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0610 - val_loss: 0.0558\n",
      "Epoch 58/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0603 - val_loss: 0.0553\n",
      "Epoch 59/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0608 - val_loss: 0.0552\n",
      "Epoch 60/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0604 - val_loss: 0.0562\n",
      "Epoch 61/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0606 - val_loss: 0.0555\n",
      "Epoch 62/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0606 - val_loss: 0.0551\n",
      "Epoch 63/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0604 - val_loss: 0.0547\n",
      "Epoch 64/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0606 - val_loss: 0.0555\n",
      "Epoch 65/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0602 - val_loss: 0.0561\n",
      "Epoch 66/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0602 - val_loss: 0.0576\n",
      "Epoch 67/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0607 - val_loss: 0.0554\n",
      "Epoch 68/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0603 - val_loss: 0.0554\n",
      "Epoch 69/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0604 - val_loss: 0.0553\n",
      "Epoch 70/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0603 - val_loss: 0.0551\n",
      "Epoch 71/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0606 - val_loss: 0.0551\n",
      "Epoch 72/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0602 - val_loss: 0.0577\n",
      "Epoch 73/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0606 - val_loss: 0.0545\n",
      "Epoch 74/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0602 - val_loss: 0.0559\n",
      "Epoch 75/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0604 - val_loss: 0.0547\n",
      "Epoch 76/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0605 - val_loss: 0.0559\n",
      "Epoch 77/300\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.0604 - val_loss: 0.0556\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0606 - val_loss: 0.0574\n",
      "Epoch 79/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0602 - val_loss: 0.0549\n",
      "Epoch 80/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0603 - val_loss: 0.0557\n",
      "Epoch 81/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0601 - val_loss: 0.0559\n",
      "Epoch 82/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0604 - val_loss: 0.0553\n",
      "Epoch 83/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0602 - val_loss: 0.0554\n",
      "Epoch 84/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0600 - val_loss: 0.0565\n",
      "Epoch 85/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0605 - val_loss: 0.0542\n",
      "Epoch 86/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0600 - val_loss: 0.0544\n",
      "Epoch 87/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0602 - val_loss: 0.0549\n",
      "Epoch 88/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0606 - val_loss: 0.0556\n",
      "Epoch 89/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0603 - val_loss: 0.0552\n",
      "Epoch 90/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0597 - val_loss: 0.0565\n",
      "Epoch 91/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0602 - val_loss: 0.0544\n",
      "Epoch 92/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0603 - val_loss: 0.0556\n",
      "Epoch 93/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0602 - val_loss: 0.0545\n",
      "Epoch 94/300\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.0598 - val_loss: 0.0555\n",
      "Epoch 95/300\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0603 - val_loss: 0.0542A:\n",
      "Epoch 96/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0603 - val_loss: 0.0573\n",
      "Epoch 97/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0601 - val_loss: 0.0554\n",
      "Epoch 98/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0602 - val_loss: 0.0543\n",
      "Epoch 99/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0599 - val_loss: 0.0549\n",
      "Epoch 100/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0603 - val_loss: 0.0552\n",
      "Epoch 101/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0603 - val_loss: 0.0550\n",
      "Epoch 102/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0597 - val_loss: 0.0559\n",
      "Epoch 103/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0596 - val_loss: 0.0554\n",
      "Epoch 104/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0600 - val_loss: 0.0553\n",
      "Epoch 105/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0597 - val_loss: 0.0549\n",
      "Epoch 106/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0598 - val_loss: 0.0556\n",
      "Epoch 107/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0603 - val_loss: 0.0552\n",
      "Epoch 108/300\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.0607 - val_loss: 0.0549E\n",
      "Epoch 109/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0596 - val_loss: 0.0545\n",
      "Epoch 110/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0601 - val_loss: 0.0548\n",
      "Epoch 111/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0597 - val_loss: 0.0554\n",
      "Epoch 112/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0604 - val_loss: 0.0552\n",
      "Epoch 113/300\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0598 - val_loss: 0.0549\n",
      "Epoch 114/300\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.0599 - val_loss: 0.0565\n",
      "Epoch 115/300\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.0599Restoring model weights from the end of the best epoch\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0599 - val_loss: 0.0553\n",
      "Epoch 00115: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, epochs =    EPOCHS,\n",
    "                                            steps_per_epoch=200,\n",
    "                                            validation_data=val_data,\n",
    "                                            validation_steps=50,\n",
    "                                            callbacks=[es]\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.054\n",
      "Test RMSE: 0.082\n"
     ]
    }
   ],
   "source": [
    "y = model.predict(x_test)\n",
    "mae = mean_absolute_error(y_test, y)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y))\n",
    "print('Test MAE: %.3f' %mae)\n",
    "print('Test RMSE: %.3f' %rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('weights.h5')    #saves the model in a .h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the saved .h5 file for predictions\n",
    "\n",
    "#load the model\n",
    "saved_model = tf.keras.models.load_model('weights.h5')\n",
    "#can be used for predictions after scaling the input data\n",
    "predicted_output = saved_model.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
