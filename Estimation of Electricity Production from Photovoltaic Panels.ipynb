{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation of Electricity Production from Photovoltaic Panels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the use of LSTM based Neural Network for estimation of electricity produced from Photovoltaic Panels based on weather data.\n",
    "<br>\n",
    "This is covered in two parts:\n",
    "<br>\n",
    "- Prediction of Irradiance\n",
    "- Prediction of Power Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset contains weather data such as Temperature, Pressure, Humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "\n",
    "PATH = 'PATH_TO_INPUT FILE'\n",
    "\n",
    "df = pd.read_csv(PATH)\n",
    "df['TimeStamp'] = pd.to_datetime(df['TimeStamp'])\n",
    "df['year'] = df['TimeStamp'].dt.year\n",
    "df['date'] = df['TimeStamp'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data\n",
    "Data is divided into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[(df.year < 2015)]\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df.year == 2015]\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "Data is scaled using the Min-Max Scaler. All the features are scaled between the range [0,1]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the data\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled = train\n",
    "test_scaled = test\n",
    "\n",
    "train_scaled[['sunHour', 'uvIndex.1', 'FeelsLikeC', 'HeatIndexC', 'cloudcover', 'humidity', 'pressure', 'tempC', 'visibility', 'day_of_year','hour_of_day']] = scaler.fit_transform(train[['sunHour', 'uvIndex.1', 'FeelsLikeC', 'HeatIndexC', 'cloudcover', 'humidity', 'pressure', 'tempC', 'visibility', 'day_of_year','hour_of_day']])\n",
    "test_scaled[['sunHour', 'uvIndex.1', 'FeelsLikeC', 'HeatIndexC', 'cloudcover', 'humidity', 'pressure', 'tempC', 'visibility', 'day_of_year','hour_of_day']] = scaler.transform(test[['sunHour', 'uvIndex.1', 'FeelsLikeC', 'HeatIndexC', 'cloudcover', 'humidity', 'pressure', 'tempC', 'visibility', 'day_of_year','hour_of_day']])\n",
    "\n",
    "yscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled[['dc_pow']] = yscaler.fit_transform(train[['dc_pow']])\n",
    "test_scaled[['dc_pow']] = yscaler.transform(test[['dc_pow']])\n",
    "\n",
    "Irrscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled[['Irr']] = Irrscaler.fit_transform(train[['Irr']])\n",
    "test_scaled[['Irr']] = Irrscaler.transform(test[['Irr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating features and labels for irradiance prediction\n",
    "\n",
    "#training data\n",
    "trainf = train_scaled[['uvIndex.1', 'cloudcover', 'humidity', 'tempC', 'visibility', 'day_of_year','hour_of_day',  'Irr']].copy()\n",
    "traint = train_scaled[['Irr']]\n",
    "\n",
    "train_dataset = trainf.values\n",
    "train_target = traint.values\n",
    "\n",
    "#testing data\n",
    "testf = test_scaled[['uvIndex.1', 'cloudcover', 'humidity', 'tempC', 'visibility', 'day_of_year','hour_of_day', 'Irr']].copy()\n",
    "testt = test_scaled[['Irr']]\n",
    "\n",
    "test_dataset = testf.values\n",
    "test_target = testt.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing the Dataset\n",
    "The data is windowed into input and output components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_dataset(dataset, target, history_size,\n",
    "                      target_size):\n",
    "    \n",
    "    '''\n",
    "    The LSTM model makes predictions (target) based on a window of consecutive samples from the data (dataset)\n",
    "    history_size specifies the number of past samples to be considered for predictions\n",
    "    target_size specifies the time offset between past sample and predictions\n",
    "    '''\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(history_size, len(dataset)-target_size):\n",
    "        indices = range(i-history_size, i, 1)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        labels.append(target[i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY = 1\n",
    "TARGET = 0\n",
    "\n",
    "x_train, y_train = window_dataset(train_dataset, train_target, HISTORY, TARGET)\n",
    "x_test, y_test = window_dataset(test_dataset, test_target, HISTORY, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data into TensorFlow Dataset to feed it into TensorFlow Model\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "print ('Single window of past history : {}'.format(x_train[0].shape))\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_data = val_data.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM based Neural Network for Irradiance Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network for irradiance prediction\n",
    "EPOCHS = 300\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=60, min_delta=0.0001, verbose=1,restore_best_weights=True)\n",
    "\n",
    "Irr_model = tf.keras.models.Sequential()\n",
    "Irr_model.add(tf.keras.layers.LSTM(32, input_shape=x_train.shape[-2:], return_sequences=True))\n",
    "Irr_model.add(tf.keras.layers.LSTM(32, activation=\"relu\"))\n",
    "Irr_model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n",
    "Irr_model.add(tf.keras.layers.Dense(8, activation=\"relu\"))\n",
    "Irr_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "Irr_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Irr_history = Irr_model.fit(train_data, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=200,\n",
    "                                            validation_data=val_data,\n",
    "                                            validation_steps=50,\n",
    "                                            callbacks=[es]\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the predicted irradiace values to the dataset\n",
    "\n",
    "y = Irr_model.predict(x_test)\n",
    "Irr_cal = Irr_model.predict(x_train)\n",
    "\n",
    "test_scaled.drop([0], inplace = True)\n",
    "train_scaled.drop([0], inplace = True)\n",
    "\n",
    "test_scaled['Irr_cal'] = y\n",
    "train_scaled['Irr_cal'] = Irr_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling data for hourly predictions\n",
    "\n",
    "train_scaled = train_scaled.groupby(['date','hour_of_day']).first().reset_index()\n",
    "test_scaled = test_scaled.groupby(['date','hour_of_day']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating features and labels for hourly power production\n",
    "\n",
    "xtrain = train_scaled[['Irr_cal', 'uvIndex.1', 'tempC', 'cloudcover', 'humidity', 'day_of_year','hour_of_day', 'dc_pow']].copy()\n",
    "ytrain = train_scaled[['dc_pow']].copy()\n",
    "\n",
    "xtest = test_scaled[['Irr_cal', 'uvIndex.1', 'tempC', 'cloudcover', 'humidity', 'day_of_year','hour_of_day', 'dc_pow']].copy()\n",
    "ytest = test_scaled[['dc_pow']].copy()\n",
    "\n",
    "trainx = xtrain.values\n",
    "trainy = ytrain.values\n",
    "\n",
    "testx = xtest.values\n",
    "testy = ytest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowing dataset\n",
    "\n",
    "HISTORY = 1     #to include more historical data for better predictions, increase HISTORY value\n",
    "TARGET = 0      #for predictions of power produced 3 hours later, set TARGET = 3\n",
    "\n",
    "x_train, y_train = window_dataset(trainx, trainy, HISTORY, TARGET)\n",
    "x_test, y_test = window_dataset(testx, testy, HISTORY, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data into TensorFlow Dataset to feed it into TensorFlow Model\n",
    "\n",
    "print ('Single window of past history : {}'.format(x_train[0].shape))\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_data = val_data.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM based Neural Network for Power Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM based Neural Network for hourly power production\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=30, min_delta=0.0001, verbose=1,restore_best_weights=True)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(64, input_shape=x_train.shape[-2:], return_sequences=True))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.LSTM(32, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(8, activation=\"tanh\"))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, epochs =    EPOCHS,\n",
    "                                            steps_per_epoch=200,\n",
    "                                            validation_data=val_data,\n",
    "                                            validation_steps=50,\n",
    "                                            callbacks=[es]\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(x_test)\n",
    "mae = mean_absolute_error(y_test, y)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y))\n",
    "print('Test MAE: %.3f' %mae)\n",
    "print('Test RMSE: %.3f' %rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('weights.h5')    #saves the model in a .h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the saved .h5 file for predictions\n",
    "\n",
    "#load the model\n",
    "saved_model = tf.keras.models.load_model('weights.h5')\n",
    "#can be used for predictions after scaling the input data\n",
    "predicted_output = saved_model.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
